{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53503bc0",
   "metadata": {},
   "source": [
    "# NLP Data Preprocess Examples\n",
    "- [Datasets](#datasets)\n",
    "- [Basic](#basic)\n",
    "- [Word Embedding](#word-embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "import konlpy\n",
    "import gensim\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c7123",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "- [20 Newsgroups Dataset](#20-newsgroups-dataset)\n",
    "- [Naver Movie Review Dataset](#naver-movie-review-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418419ff",
   "metadata": {},
   "source": [
    "### 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 10930\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "documents = [re.sub(\"[^a-zA-Z ]\", ' ', document) for document in documents]\n",
    "documents = [document.lower() for document in documents if len(document.split())>3]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "tokenized = [pattern.sub('', document).split() for document in documents]\n",
    "tokenized = [document for document in tokenized if len(document)>1]\n",
    "print('Samples:', len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed608a",
   "metadata": {},
   "source": [
    "### Naver Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6888623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt', filename='ratings.txt')\n",
    "data = pd.read_table('ratings.txt')\n",
    "data['document'] = data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','')\n",
    "data = data[data['document'].notna()]\n",
    "\n",
    "okt = Okt()\n",
    "stop_words = {'의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'}\n",
    "tokenized = list()\n",
    "\n",
    "for sentence in data['document']:\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    tokenized_sentence = [word for word in tokenized_sentence if word not in stop_words]\n",
    "    tokenized.append(tokenized_sentence)\n",
    "print('Samples:', len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105069a",
   "metadata": {},
   "source": [
    "### Tokenized Dataset Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ffd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value:key for key,value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6930a",
   "metadata": {},
   "source": [
    "## Basic\n",
    "- [Tokenization](#tokenization)\n",
    "- [Stemming](#stemming)\n",
    "- [Lemmatization](#lemmatization)\n",
    "- [Stopword](#stopword)\n",
    "- [Integer Encoding](#integer-encoding)\n",
    "- [Padding](#padding)\n",
    "- [One-Hot Encoding](#one-hot-encoding)\n",
    "- [Korean Tools](#korean-tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222062da",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41055f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenization: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "POS tagging: [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('Word tokenization:', tokenized_sentence)\n",
    "print('POS tagging:', pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a86bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('Sentence Tokenization:', sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6672068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석: ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅: [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출: ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "text = '열심히 코딩한 당신, 연휴에는 여행을 가봐요'\n",
    "\n",
    "print('OKT 형태소 분석:', okt.morphs(text))\n",
    "print('OKT 품사 태깅:', okt.pos(text))\n",
    "print('OKT 명사 추출:', okt.nouns(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f979225",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ced102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:          ['formalize', 'allowance', 'electricical']\n",
      "Porter Stemmer:    ['formal', 'allow', 'electric']\n",
      "Lancaster Stemmer: ['form', 'allow', 'elect']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('Original:'.ljust(18,' '), words)\n",
    "print('Porter Stemmer:'.ljust(18,' '), [porter_stemmer.stem(w) for w in words])\n",
    "print('Lancaster Stemmer:'.ljust(18,' '), [lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6e308",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1deb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:      ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "Lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('Original:'.ljust(14,' '), words)\n",
    "print('Lemmatization:'.ljust(14,' '), [lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230feff",
   "metadata": {},
   "source": [
    "### Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_list = stopwords.words('english')\n",
    "print('Stopwords:',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origianl: ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "Result:   ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "print('Origianl:'.ljust(9,' '), word_tokens) \n",
    "print('Result:'.ljust(9,' '), [word for word in word_tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "raw_text = 'A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentences = sent_tokenize(raw_text)\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "preprocessed_sentences = list()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    word_tokens = [word.lower() for word in sentence if len(word) > 2]\n",
    "    word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "    preprocessed_sentences.append(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2b103",
   "metadata": {},
   "source": [
    "### Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts: {'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3}\n",
      "Word index: {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n",
      "Encoded sentences: [[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "\n",
    "words = np.hstack(preprocessed_sentences)\n",
    "vocab_size = 5\n",
    "vocab = dict(Counter(words).most_common(vocab_size))\n",
    "# vocab = dict(FreqDist(words).most_common(vocab_size))\n",
    "print('Word counts:', vocab)\n",
    "\n",
    "word_to_index = {word:i for i,word in enumerate(vocab.keys(),1)}\n",
    "word_to_index['OOV'] = len(word_to_index)+1 # Out-Of-Vocabulary 토큰 추가\n",
    "print('Word index:', word_to_index)\n",
    "\n",
    "encoded_sentences = list()\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = [word_to_index[word] if word in word_to_index else word_to_index['OOV'] for word in sentence]\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "print('Encoded sentences:', encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b159479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts: {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "Word index: {'OOV': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'kept': 5, 'person': 6, 'word': 7, 'keeping': 8, 'good': 9, 'knew': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n",
      "Encoded sentences: [[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words=vocab_size+2, oov_token='OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "\n",
    "# num_words는 texts_to_sequences()에만 적용\n",
    "print('Word counts:', dict(tokenizer.word_counts))\n",
    "print('Word index:', tokenizer.word_index)\n",
    "print('Encoded sentences:', encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb2a81",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f4ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0],\n",
       "       [2, 4, 6, 0, 0],\n",
       "       [1, 3, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0],\n",
       "       [4, 3, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 3, 0, 0],\n",
       "       [1, 1, 4, 3, 1],\n",
       "       [2, 1, 4, 1, 0]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083a291",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8438ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endoced: [2, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print('endoced:', encoded[0])\n",
    "\n",
    "one_hot = to_categorical(encoded[0])\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2844f9",
   "metadata": {},
   "source": [
    "### Korean Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b30717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = str()\n",
    "\n",
    "# PyKoSpacing\n",
    "# 띄어쓰기 교정\n",
    "# pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "\n",
    "from pykospacing import Spacing\n",
    "\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(sentence)\n",
    "\n",
    "# Py-Hanspell\n",
    "# 네이버 한글 맞춤법 검사기\n",
    "# pip install git+https://github.com/ssut/py-hanspell.git\n",
    "\n",
    "from hanspell import spell_checker\n",
    "\n",
    "spelled_sent = spell_checker.check(sentence)\n",
    "hanspell_sent = spelled_sent.checked\n",
    "\n",
    "# SOYNLP\n",
    "# 학습 기반 토크나이저, 반복되는 문자 정제\n",
    "# pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffaf1ad",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "- [DTM](#dtm)\n",
    "- [TF-IDF](#tf-idf)\n",
    "- [Word2Vec](#word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5029f6",
   "metadata": {},
   "source": [
    "### DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a52715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', 'I like you', 'what should I do ']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5e45e",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24adb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', 'I like you', 'what should I do ']\n",
    "vector = TfidfVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9b0da",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=tokenized, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nice', 0.6341156363487244), ('better', 0.5764698386192322), ('probably', 0.572699248790741), ('prefer', 0.5555704236030579), ('think', 0.5463138222694397), ('want', 0.5313563346862793), ('looking', 0.5295560359954834), ('suspiciously', 0.5255772471427917), ('bad', 0.5236798524856567), ('guess', 0.5195480585098267)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('like')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v') # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af08bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e433785c256eddf1a97ef126132771859332314632db55c9adf99951004659"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
