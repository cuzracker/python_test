{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53503bc0",
   "metadata": {},
   "source": [
    "# NLP Data Preprocess Examples\n",
    "- [Datasets](#datasets)\n",
    "- [Basic](#basic)\n",
    "- [Word Embedding](#word-embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "import konlpy\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c7123",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "- [20 Newsgroups Dataset](#20-newsgroups-dataset)\n",
    "- [SMS Spam Collection Dataset](#sms-spam-collection-dataset)\n",
    "- [Book Description Dataset](#book-description-dataset)\n",
    "- [Naver Movie Review Dataset](#naver-movie-review-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418419ff",
   "metadata": {},
   "source": [
    "### 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "print(len(dataset.data))\n",
    "dataset.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306b0e4",
   "metadata": {},
   "source": [
    "### SMS Spam Collection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', filename='spam.csv')\n",
    "dataset = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7188491",
   "metadata": {},
   "source": [
    "### Book Description Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2382, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>genre</th>\n",
       "      <th>image_link</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We know that power is shifting: From West to E...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Moisés Naím</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>3.63</td>\n",
       "      <td>The End of Power: From Boardrooms to Battlefie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Following the success of The Accidental Billio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Blake J. Harris</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>3.94</td>\n",
       "      <td>Console Wars: Sega, Nintendo, and the Battle t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How to tap the power of social software and ne...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Chris Brogan</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>3.78</td>\n",
       "      <td>Trust Agents: Using the Web to Build Influence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>William J. Bernstein is an American financial ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>William J. Bernstein</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>4.20</td>\n",
       "      <td>The Four Pillars of Investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Amazing book. And I joined Steve Jobs and many...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Akio Morita</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>4.05</td>\n",
       "      <td>Made in Japan: Akio Morita and Sony</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                                               Desc  \\\n",
       "0             0  We know that power is shifting: From West to E...   \n",
       "1             1  Following the success of The Accidental Billio...   \n",
       "2             2  How to tap the power of social software and ne...   \n",
       "3             3  William J. Bernstein is an American financial ...   \n",
       "4             4  Amazing book. And I joined Steve Jobs and many...   \n",
       "\n",
       "   Unnamed: 0                author     genre  \\\n",
       "0         0.0           Moisés Naím  Business   \n",
       "1         1.0       Blake J. Harris  Business   \n",
       "2         2.0          Chris Brogan  Business   \n",
       "3         3.0  William J. Bernstein  Business   \n",
       "4         4.0           Akio Morita  Business   \n",
       "\n",
       "                                          image_link  rating  \\\n",
       "0  https://i.gr-assets.com/images/S/compressed.ph...    3.63   \n",
       "1  https://i.gr-assets.com/images/S/compressed.ph...    3.94   \n",
       "2  https://i.gr-assets.com/images/S/compressed.ph...    3.78   \n",
       "3  https://i.gr-assets.com/images/S/compressed.ph...    4.20   \n",
       "4  https://i.gr-assets.com/images/S/compressed.ph...    4.05   \n",
       "\n",
       "                                               title  \n",
       "0  The End of Power: From Boardrooms to Battlefie...  \n",
       "1  Console Wars: Sega, Nintendo, and the Battle t...  \n",
       "2  Trust Agents: Using the Web to Build Influence...  \n",
       "3                      The Four Pillars of Investing  \n",
       "4                Made in Japan: Akio Morita and Sony  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/data.csv\", filename=\"data.csv\")\n",
    "dataset = pd.read_csv(\"data.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed608a",
   "metadata": {},
   "source": [
    "### Naver Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6888623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt', filename='ratings.txt')\n",
    "dataset = pd.read_table('ratings.txt')\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6585d",
   "metadata": {},
   "source": [
    "### English Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 10930\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "documents = dataset.data\n",
    "\n",
    "documents = [re.sub('[^a-zA-Z ]', ' ', document) for document in documents]\n",
    "documents = [document.lower() for document in documents if len(document.split())>3]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "tokenized = [pattern.sub('', document).split() for document in documents]\n",
    "tokenized = [document for document in tokenized if len(document)>1]\n",
    "print('Samples:', len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cff832",
   "metadata": {},
   "source": [
    "### Korean Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de69ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "documents = list()\n",
    "\n",
    "documents = [re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', document) for document in documents]\n",
    "documents = [document for document in documents if len(document.split())>3]\n",
    "\n",
    "okt = Okt()\n",
    "stop_words = stopwords.words('korean')\n",
    "tokenized = list()\n",
    "\n",
    "for document in documents:\n",
    "    tokenized_sentence = okt.morphs(document, stem=True)\n",
    "    tokenized_sentence = [word for word in tokenized_sentence if word not in stop_words]\n",
    "    tokenized.append(tokenized_sentence)\n",
    "print('Samples:', len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105069a",
   "metadata": {},
   "source": [
    "### Tokenized Dataset Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ffd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value:key for key,value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6930a",
   "metadata": {},
   "source": [
    "## Basic\n",
    "- [Tokenization](#tokenization)\n",
    "- [Stemming](#stemming)\n",
    "- [Lemmatization](#lemmatization)\n",
    "- [Stopword](#stopword)\n",
    "- [Integer Encoding](#integer-encoding)\n",
    "- [Padding](#padding)\n",
    "- [One-Hot Encoding](#one-hot-encoding)\n",
    "- [Korean Tools](#korean-tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222062da",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41055f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenization: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "POS tagging: [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('Word tokenization:', tokenized_sentence)\n",
    "print('POS tagging:', pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a86bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('Sentence Tokenization:', sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6672068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석: ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅: [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출: ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "text = '열심히 코딩한 당신, 연휴에는 여행을 가봐요'\n",
    "\n",
    "print('OKT 형태소 분석:', okt.morphs(text))\n",
    "print('OKT 품사 태깅:', okt.pos(text))\n",
    "print('OKT 명사 추출:', okt.nouns(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f979225",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ced102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:          ['formalize', 'allowance', 'electricical']\n",
      "Porter Stemmer:    ['formal', 'allow', 'electric']\n",
      "Lancaster Stemmer: ['form', 'allow', 'elect']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('Original:'.ljust(18,' '), words)\n",
    "print('Porter Stemmer:'.ljust(18,' '), [porter_stemmer.stem(w) for w in words])\n",
    "print('Lancaster Stemmer:'.ljust(18,' '), [lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6e308",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1deb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:      ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "Lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('Original:'.ljust(14,' '), words)\n",
    "print('Lemmatization:'.ljust(14,' '), [lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230feff",
   "metadata": {},
   "source": [
    "### Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_list = stopwords.words('english')\n",
    "print('Stopwords:',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origianl: ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "Result:   ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "print('Origianl:'.ljust(9,' '), word_tokens) \n",
    "print('Result:'.ljust(9,' '), [word for word in word_tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "raw_text = 'A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentences = sent_tokenize(raw_text)\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "preprocessed_sentences = list()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    word_tokens = [word.lower() for word in sentence if len(word) > 2]\n",
    "    word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "    preprocessed_sentences.append(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2b103",
   "metadata": {},
   "source": [
    "### Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts: {'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3}\n",
      "Word index: {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n",
      "Encoded sentences: [[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "\n",
    "words = np.hstack(preprocessed_sentences)\n",
    "vocab_size = 5\n",
    "vocab = dict(Counter(words).most_common(vocab_size))\n",
    "# vocab = dict(FreqDist(words).most_common(vocab_size))\n",
    "print('Word counts:', vocab)\n",
    "\n",
    "word_to_index = {word:i for i,word in enumerate(vocab.keys(),1)}\n",
    "word_to_index['OOV'] = len(word_to_index)+1 # Out-Of-Vocabulary 토큰 추가\n",
    "print('Word index:', word_to_index)\n",
    "\n",
    "encoded_sentences = list()\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = [word_to_index[word] if word in word_to_index else word_to_index['OOV'] for word in sentence]\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "print('Encoded sentences:', encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b159479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts: {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "Word index: {'OOV': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'kept': 5, 'person': 6, 'word': 7, 'keeping': 8, 'good': 9, 'knew': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n",
      "Encoded sentences: [[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words=vocab_size+2, oov_token='OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "\n",
    "# num_words는 texts_to_sequences()에만 적용\n",
    "print('Word counts:', dict(tokenizer.word_counts))\n",
    "print('Word index:', tokenizer.word_index)\n",
    "print('Encoded sentences:', encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb2a81",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f4ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0],\n",
       "       [2, 4, 6, 0, 0],\n",
       "       [1, 3, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0],\n",
       "       [4, 3, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 1, 0, 0],\n",
       "       [2, 5, 3, 0, 0],\n",
       "       [1, 1, 4, 3, 1],\n",
       "       [2, 1, 4, 1, 0]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083a291",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8438ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endoced: [2, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print('endoced:', encoded[0])\n",
    "\n",
    "one_hot = to_categorical(encoded[0])\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2844f9",
   "metadata": {},
   "source": [
    "### Korean Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b30717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyKoSpacing\n",
    "# 띄어쓰기 교정\n",
    "# pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "\n",
    "from pykospacing import Spacing\n",
    "\n",
    "sentence = str()\n",
    "\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(sentence)\n",
    "\n",
    "# Py-Hanspell\n",
    "# 네이버 한글 맞춤법 검사기\n",
    "# pip install git+https://github.com/ssut/py-hanspell.git\n",
    "\n",
    "from hanspell import spell_checker\n",
    "\n",
    "spelled_sent = spell_checker.check(sentence)\n",
    "hanspell_sent = spelled_sent.checked\n",
    "\n",
    "# SOYNLP\n",
    "# 학습 기반 토크나이저, 반복되는 문자 정제\n",
    "# pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffaf1ad",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "- [DTM](#dtm)\n",
    "- [TF-IDF](#tf-idf)\n",
    "- [Word2Vec](#word2vec)\n",
    "- [GloVe](#glove)\n",
    "- [FastText](#fasttext)\n",
    "- [Keras Embedding Layer](#keras-embedding-layer)\n",
    "- [ELMo](#elmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5029f6",
   "metadata": {},
   "source": [
    "### DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a52715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', 'I like you', 'what should I do ']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5e45e",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24adb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', 'I like you', 'what should I do ']\n",
    "vector = TfidfVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9b0da",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# sg=0: CBOW, sg=1: Skip-gram\n",
    "model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nice', 0.6690633893013), ('better', 0.6060903668403625), ('probably', 0.5894891619682312), ('prefer', 0.5708296298980713), ('bad', 0.5548878908157349), ('done', 0.5529392957687378), ('ferrari', 0.5524083375930786), ('want', 0.539306104183197), ('think', 0.5361883640289307), ('guess', 0.5306634306907654)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('like')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v') # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글의 사전 훈련된 Word2Vec 모델\n",
    "urllib.request.urlretrieve('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', \\\n",
    "                           filename='GoogleNews-vectors-negative300.bin.gz')\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Visualization\n",
    "# @ https://projector.tensorflow.org/\n",
    "\n",
    "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7324b6",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve, urlopen\n",
    "import zipfile\n",
    "\n",
    "# 사전 훈련된 GloVe\n",
    "urlretrieve('http://nlp.stanford.edu/data/glove.6B.zip', filename='glove.6B.zip')\n",
    "zf = zipfile.ZipFile('glove.6B.zip')\n",
    "zf.extractall()\n",
    "\n",
    "embedding_dict = dict()\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        word_vector = line.split()\n",
    "        word = word_vector[0]\n",
    "        word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "        embedding_dict[word] = word_vector_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe7211",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af08bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(sentences=tokenized, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd99786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('likes', 0.795409083366394), ('likud', 0.7720040082931519), ('liked', 0.7430700659751892), ('unlike', 0.74107825756073), ('alike', 0.7275046110153198), ('likewise', 0.7235626578330994), ('dislike', 0.6973028182983398), ('nike', 0.687152087688446), ('pike', 0.6598094701766968), ('unhappy', 0.6429505944252014)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('like')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670554b5",
   "metadata": {},
   "source": [
    "### Keras Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "vocab_size = 20000\n",
    "output_dim = 128\n",
    "input_length = 500\n",
    "\n",
    "v = Embedding(vocab_size, output_dim, input_length=input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83912ab5",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb51afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "\n",
    "def ELMoEmbedding(x):\n",
    "    return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\"default\")[\"default\"]\n",
    "\n",
    "# embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e433785c256eddf1a97ef126132771859332314632db55c9adf99951004659"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
